#!/usr/bin/env python3
"""
GitHub Sentinel v0.2 æµ‹è¯•è„šæœ¬ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé¿å…tokené™åˆ¶ï¼‰
"""
import asyncio
import os
import sys
from pathlib import Path
from datetime import datetime, timedelta, timezone

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.services.github_service import GitHubService
from src.services.llm_service import LLMService, create_azure_openai_provider
from src.services.report_service import ReportService

async def test_v02_features():
    """æµ‹è¯•v0.2åŠŸèƒ½"""
    print("ğŸš€ GitHub Sentinel v0.2 åŠŸèƒ½æµ‹è¯• (ä¼˜åŒ–ç‰ˆæœ¬)")

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    github_token = os.getenv("GITHUB_TOKEN")
    if not github_token:
        print("âŒ è¯·è®¾ç½® GITHUB_TOKEN ç¯å¢ƒå˜é‡")
        return

    # åˆå§‹åŒ–GitHubæœåŠ¡
    github_service = GitHubService(token=github_token)
    print("âœ… GitHubæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

    # åˆå§‹åŒ–LLMæœåŠ¡
    llm_service = LLMService()

    # å¦‚æœé…ç½®äº†Azure OpenAIï¼Œæ·»åŠ æä¾›å•†
    azure_key = os.getenv("AZURE_OPENAI_API_KEY")
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

    if azure_key and azure_endpoint:
        try:
            provider = create_azure_openai_provider({
                'model_name': os.getenv("AZURE_OPENAI_MODEL", "gpt-4"),
                'api_key': azure_key,
                'azure_endpoint': azure_endpoint,
                'api_version': os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
            })
            llm_service.add_provider("azure_openai", provider, is_default=True)
            print("âœ… Azure OpenAIæä¾›å•†é…ç½®æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  Azure OpenAIé…ç½®å¤±è´¥: {e}")

    # åˆå§‹åŒ–æŠ¥å‘ŠæœåŠ¡
    report_service = ReportService(llm_service, github_service)
    print("âœ… æŠ¥å‘ŠæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

    # æµ‹è¯•åŠŸèƒ½
    try:
        print("\nğŸ“Š æµ‹è¯•åŠŸèƒ½1: ç”Ÿæˆç´§å‡‘æ¨¡å¼è¿›å±•æŠ¥å‘Šï¼ˆèŠ‚çœtokenï¼‰")

        # ä½¿ç”¨è¾ƒçŸ­çš„æ—¶é—´èŒƒå›´ï¼ˆ12å°æ—¶ï¼‰å’Œç´§å‡‘æ¨¡å¼
        until = datetime.now(timezone.utc)
        since = until - timedelta(hours=12)  # ç¼©çŸ­åˆ°12å°æ—¶

        progress_file = await report_service.generate_daily_progress_report(
            "microsoft", "vscode",
            since=since,
            until=until,
            compact_mode=True  # ä½¿ç”¨ç´§å‡‘æ¨¡å¼
        )
        print(f"âœ… è¿›å±•æŠ¥å‘Šå·²ç”Ÿæˆ: {progress_file}")

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        with open(progress_file, 'r', encoding='utf-8') as f:
            content = f.read()
            print(f"ğŸ“ æŠ¥å‘Šå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

        if llm_service.list_providers():
            print("\nğŸ¤– æµ‹è¯•åŠŸèƒ½2: ç”ŸæˆLLMæ‘˜è¦æŠ¥å‘Šï¼ˆä½¿ç”¨è¾ƒå°tokenæ•°é‡ï¼‰")

            # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°è®¾ç½®
            summary_file = await report_service.generate_llm_summary_report(
                "vscode",
                progress_file,
                max_tokens=1000,  # é™ä½åˆ°1000 tokens
                temperature=0.5   # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
            )
            print(f"âœ… æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_file}")

            # è¯»å–å¹¶æ˜¾ç¤ºæ‘˜è¦å†…å®¹ç‰‡æ®µ
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary_content = f.read()
                print(f"ğŸ“„ æ‘˜è¦æŠ¥å‘Šé¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰:")
                print(f"   {summary_content[:200]}...")

        else:
            print("âš ï¸  è·³è¿‡LLMæµ‹è¯•ï¼ˆæœªé…ç½®LLMæä¾›å•†ï¼‰")

        print("\nğŸ¯ ä¼˜åŒ–å»ºè®®:")
        print("1. ä½¿ç”¨ç´§å‡‘æ¨¡å¼å‡å°‘å†…å®¹é•¿åº¦")
        print("2. ç¼©çŸ­æ—¶é—´èŒƒå›´åˆ°12-24å°æ—¶")
        print("3. è®¾ç½®è¾ƒå°çš„max_tokenså€¼ (1000-1500)")
        print("4. é™ä½temperatureå€¼ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º")
        print("5. åªå…³æ³¨é‡è¦ä¿¡æ¯ï¼ˆå·²åˆå¹¶PRå’Œæ´»è·ƒIssuesï¼‰")

    except Exception as e:
        error_msg = str(e)
        if "429" in error_msg and "rate limit" in error_msg.lower():
            print("âŒ é‡åˆ°Azure OpenAIé€Ÿç‡é™åˆ¶é—®é¢˜")
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print("   1. ç­‰å¾…å‡ åˆ†é’Ÿåé‡è¯•")
            print("   2. ä½¿ç”¨æ›´å°çš„æ—¶é—´èŒƒå›´ (--hours 6)")
            print("   3. ä½¿ç”¨æ›´å°çš„max_tokenså€¼ (--max-tokens 800)")
            print("   4. å‡çº§åˆ°æ›´é«˜çº§åˆ«çš„Azure OpenAIå®šä»·å±‚")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

async def test_minimal_example():
    """æµ‹è¯•æœ€å°åŒ–ç¤ºä¾‹ï¼ˆæå°‘tokenä½¿ç”¨ï¼‰"""
    print("\nğŸ§ª æµ‹è¯•æœ€å°åŒ–ç¤ºä¾‹ï¼ˆé€‚ç”¨äºS0å®šä»·å±‚ï¼‰")

    github_token = os.getenv("GITHUB_TOKEN")
    if not github_token:
        print("âŒ è¯·è®¾ç½® GITHUB_TOKEN ç¯å¢ƒå˜é‡")
        return

    try:
        github_service = GitHubService(token=github_token)

        # åªè·å–è¿‡å»6å°æ—¶çš„å·²åˆå¹¶PR
        until = datetime.now(timezone.utc)
        since = until - timedelta(hours=6)

        # ç›´æ¥è°ƒç”¨GitHub APIè·å–æœ€å°‘çš„æ•°æ®
        pull_requests = await github_service.get_pull_requests(
            "microsoft", "vscode",
            since=since,
            until=until,
            per_page=5,  # åªè·å–5ä¸ªPR
            merged_only=True,  # åªè¦å·²åˆå¹¶çš„
            include_body=False  # ä¸åŒ…å«è¯¦ç»†æè¿°
        )

        print(f"âœ… è·å–åˆ° {len(pull_requests)} ä¸ªå·²åˆå¹¶çš„PR")
        for pr in pull_requests[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"   - #{pr['number']}: {pr['title'][:50]}...")

        # å¦‚æœæœ‰PRï¼Œåˆ›å»ºä¸€ä¸ªæç®€æŠ¥å‘Š
        if pull_requests:
            simple_report = f"""# vscode ç®€è¦è¿›å±•

è¿‡å»6å°æ—¶å·²åˆå¹¶çš„PR ({len(pull_requests)}ä¸ª):
{chr(10).join([f"- #{pr['number']}: {pr['title']}" for pr in pull_requests[:3]])}
"""
            print(f"ğŸ“ ç”Ÿæˆçš„ç®€è¦æŠ¥å‘Š:")
            print(simple_report)

    except Exception as e:
        print(f"âŒ æœ€å°åŒ–æµ‹è¯•å¤±è´¥: {e}")

def test_cli_integration():
    """æµ‹è¯•CLIé›†æˆ"""
    print("\nğŸ”§ æµ‹è¯•CLIé›†æˆ")

    # æµ‹è¯•CLIå‘½ä»¤å¸®åŠ©
    try:
        import subprocess

        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥é¿å…ç¼–ç é—®é¢˜
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'

        result = subprocess.run([
            sys.executable, "-m", "src.cli.commands", "--help"
        ], capture_output=True, text=True, cwd=project_root,
           encoding='utf-8', errors='replace', env=env)

        if result.returncode == 0:
            print("âœ… CLIå‘½ä»¤å¸®åŠ©æ­£å¸¸")
            print("   å¯ç”¨å‘½ä»¤åŒ…æ‹¬: progress, summary, report, batch, llm, history")
        else:
            print(f"âŒ CLIå‘½ä»¤æµ‹è¯•å¤±è´¥:")
            if result.stderr:
                # å®‰å…¨åœ°å¤„ç†é”™è¯¯è¾“å‡ºï¼Œé¿å…ç¼–ç é—®é¢˜
                error_msg = result.stderr.encode('utf-8', errors='replace').decode('utf-8')
                print(f"   é”™è¯¯ä¿¡æ¯: {error_msg[:200]}...")
            if result.stdout:
                stdout_msg = result.stdout.encode('utf-8', errors='replace').decode('utf-8')
                print(f"   è¾“å‡ºä¿¡æ¯: {stdout_msg[:200]}...")

        # æµ‹è¯•LLMæä¾›å•†åˆ—è¡¨
        print("ğŸ” æµ‹è¯•LLMæä¾›å•†åˆ—è¡¨...")
        result = subprocess.run([
            sys.executable, "-m", "src.cli.commands", "llm", "list"
        ], capture_output=True, text=True, cwd=project_root,
           encoding='utf-8', errors='replace', env=env)

        if result.returncode == 0:
            print("âœ… LLMæä¾›å•†åˆ—è¡¨æ­£å¸¸")
            # å®‰å…¨åœ°æ˜¾ç¤ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†
            if result.stdout:
                output_lines = result.stdout.split('\n')
                for line in output_lines[:5]:  # åªæ˜¾ç¤ºå‰5è¡Œ
                    if line.strip():
                        print(f"   {line}")
        else:
            print(f"âŒ LLMæä¾›å•†åˆ—è¡¨å¤±è´¥:")
            if result.stderr:
                error_msg = result.stderr.encode('utf-8', errors='replace').decode('utf-8')
                print(f"   é”™è¯¯ä¿¡æ¯: {error_msg[:200]}...")

    except Exception as e:
        print(f"âŒ CLIé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        # æä¾›æ›¿ä»£çš„ç®€å•æµ‹è¯•
        print("ğŸ”„ å°è¯•ç®€åŒ–çš„CLIæµ‹è¯•...")
        try:
            # ç›´æ¥å¯¼å…¥å’Œæµ‹è¯•CLIæ¨¡å—
            from src.cli.commands import GitHubSentinelCLI
            cli = GitHubSentinelCLI()
            parser = cli.create_parser()
            print("âœ… CLIæ¨¡å—å¯¼å…¥å’Œåˆå§‹åŒ–æˆåŠŸ")

            # æµ‹è¯•è§£æå™¨
            help_text = parser.format_help()
            if "progress" in help_text and "summary" in help_text:
                print("âœ… CLIå‘½ä»¤è§£æå™¨æ­£å¸¸ï¼ŒåŒ…å«v0.2æ–°å‘½ä»¤")
            else:
                print("âš ï¸ CLIå‘½ä»¤è§£æå™¨å¯èƒ½ç¼ºå°‘æŸäº›å‘½ä»¤")

        except Exception as inner_e:
            print(f"âŒ ç®€åŒ–CLIæµ‹è¯•ä¹Ÿå¤±è´¥: {inner_e}")

if __name__ == "__main__":
    print("GitHub Sentinel v0.2 æµ‹è¯•å¥—ä»¶")
    print("=" * 50)
    print("é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. æ ‡å‡†æµ‹è¯•ï¼ˆéœ€è¦è¶³å¤Ÿçš„tokené…é¢ï¼‰")
    print("2. æœ€å°åŒ–æµ‹è¯•ï¼ˆé€‚ç”¨äºS0å®šä»·å±‚ï¼‰")
    print("3. CLIé›†æˆæµ‹è¯•")
    print("4. è¿è¡Œæ‰€æœ‰æµ‹è¯•")

    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()

    if choice == "2":
        asyncio.run(test_minimal_example())
    elif choice == "3":
        test_cli_integration()
    elif choice == "4":
        print("\nğŸš€ è¿è¡Œæ‰€æœ‰æµ‹è¯•...")
        test_cli_integration()
        asyncio.run(test_minimal_example())
        asyncio.run(test_v02_features())
    else:
        asyncio.run(test_v02_features())
