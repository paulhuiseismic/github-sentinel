"""
GitHub API æœåŠ¡
"""
import aiohttp
import asyncio
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta, timezone
import logging
import json
import os
from pathlib import Path

from ..models.repository import Repository, RepositoryUpdate


def parse_github_datetime(date_string: str) -> datetime:
    """è§£æGitHub APIè¿”å›çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œç¡®ä¿è¿”å›timezone-awareçš„datetime"""
    if date_string.endswith('Z'):
        # GitHub APIé€šå¸¸è¿”å›UTCæ—¶é—´ï¼Œä»¥'Z'ç»“å°¾
        date_string = date_string.replace('Z', '+00:00')

    try:
        dt = datetime.fromisoformat(date_string)
        # ç¡®ä¿æ˜¯timezone-awareçš„
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except ValueError:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›å½“å‰UTCæ—¶é—´
        return datetime.now(timezone.utc)


def ensure_utc_datetime(dt: datetime) -> datetime:
    """ç¡®ä¿datetimeæ˜¯UTCæ—¶åŒºçš„timezone-awareå¯¹è±¡"""
    if dt is None:
        return None

    if dt.tzinfo is None:
        # å¦‚æœæ˜¯naive datetimeï¼Œå‡è®¾å®ƒæ˜¯UTCæ—¶é—´
        return dt.replace(tzinfo=timezone.utc)
    else:
        # å¦‚æœå·²ç»æœ‰æ—¶åŒºä¿¡æ¯ï¼Œè½¬æ¢ä¸ºUTC
        return dt.astimezone(timezone.utc)


class GitHubService:
    """GitHub API æœåŠ¡ç±»"""

    def __init__(self, token: str, rate_limit_per_hour: int = 5000, timeout: int = 30):
        self.token = token
        self.base_url = "https://api.github.com"
        self.headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
            "User-Agent": "GitHub-Sentinel/1.0"
        }
        self.rate_limit_per_hour = rate_limit_per_hour
        self.requests_made = 0
        self.last_reset = datetime.now(timezone.utc)
        self.timeout = timeout
        self.logger = logging.getLogger(__name__)

    async def _check_rate_limit(self):
        """æ£€æŸ¥APIé€Ÿç‡é™åˆ¶"""
        now = datetime.now(timezone.utc)
        if (now - self.last_reset).total_seconds() > 3600:
            self.requests_made = 0
            self.last_reset = now

        if self.requests_made >= self.rate_limit_per_hour:
            wait_time = 3600 - (now - self.last_reset).total_seconds()
            if wait_time > 0:
                self.logger.warning(f"è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.0f} ç§’")
                await asyncio.sleep(wait_time)
                self.requests_made = 0
                self.last_reset = datetime.now(timezone.utc)

    async def _make_request(self, url: str, params: Optional[Dict] = None) -> Dict:
        """å‘èµ·APIè¯·æ±‚"""
        await self._check_rate_limit()

        timeout = aiohttp.ClientTimeout(total=self.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                async with session.get(url, headers=self.headers, params=params) as response:
                    self.requests_made += 1

                    if response.status == 200:
                        return await response.json()
                    elif response.status == 403:
                        self.logger.error(f"APIè®¿é—®è¢«æ‹’ç»: {response.status}")
                        raise Exception(f"GitHub APIè®¿é—®è¢«æ‹’ç»: {response.status}")
                    elif response.status == 404:
                        self.logger.error(f"èµ„æºæœªæ‰¾åˆ°: {url}")
                        raise Exception(f"GitHubèµ„æºæœªæ‰¾åˆ°: {url}")
                    else:
                        self.logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status}")
                        raise Exception(f"GitHub APIè¯·æ±‚å¤±è´¥: {response.status}")

            except asyncio.TimeoutError:
                self.logger.error(f"è¯·æ±‚è¶…æ—¶: {url}")
                raise Exception(f"GitHub APIè¯·æ±‚è¶…æ—¶: {url}")
            except Exception as e:
                self.logger.error(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")
                raise

    async def get_repository_info(self, owner: str, repo: str) -> Repository:
        """è·å–ä»“åº“åŸºæœ¬ä¿¡æ¯"""
        url = f"{self.base_url}/repos/{owner}/{repo}"
        data = await self._make_request(url)

        return Repository(
            name=data['name'],
            full_name=data['full_name'],
            description=data.get('description', ''),
            html_url=data['html_url'],
            language=data.get('language', ''),
            stars=data['stargazers_count'],
            forks=data['forks_count'],
            updated_at=parse_github_datetime(data['updated_at']),
            created_at=parse_github_datetime(data['created_at'])
        )

    async def get_repository_updates(self, owner: str, repo: str, since: datetime) -> List[RepositoryUpdate]:
        """è·å–ä»“åº“æ›´æ–°ä¿¡æ¯"""
        # ç¡®ä¿sinceå‚æ•°æ˜¯timezone-awareçš„
        since = ensure_utc_datetime(since)

        # è·å–æœ€æ–°çš„commits
        commits_url = f"{self.base_url}/repos/{owner}/{repo}/commits"
        params = {
            'since': since.isoformat(),
            'per_page': 100
        }

        commits_data = await self._make_request(commits_url, params)

        updates = []
        for commit in commits_data:
            updates.append(RepositoryUpdate(
                type='commit',
                title=commit['commit']['message'].split('\n')[0],
                description=commit['commit']['message'],
                author=commit['commit']['author']['name'],
                created_at=parse_github_datetime(commit['commit']['author']['date']),
                html_url=commit['html_url']
            ))

        return updates

    async def get_issues(self, owner: str, repo: str, since: Optional[datetime] = None,
                        until: Optional[datetime] = None, state: str = "all",
                        per_page: int = 50, include_body: bool = False) -> List[Dict]:
        """è·å–ä»“åº“çš„ issues åˆ—è¡¨"""
        url = f"{self.base_url}/repos/{owner}/{repo}/issues"
        params = {
            'state': state,
            'per_page': per_page,
            'sort': 'updated',
            'direction': 'desc'
        }

        if since:
            since = ensure_utc_datetime(since)
            params['since'] = since.isoformat()

        data = await self._make_request(url, params)

        # è¿‡æ»¤æ‰ pull requests (GitHub API ä¸­ issues åŒ…å« pull requests)
        issues = []
        for item in data:
            if 'pull_request' not in item:
                # æ—¶é—´è¿‡æ»¤
                if until:
                    until = ensure_utc_datetime(until)
                    updated_at = parse_github_datetime(item['updated_at'])
                    if updated_at > until:
                        continue

                issue_data = {
                    'number': item['number'],
                    'title': item['title'],
                    'state': item['state'],
                    'user': item['user']['login'],
                    'created_at': item['created_at'],
                    'updated_at': item['updated_at'],
                    'html_url': item['html_url'],
                    'labels': [label['name'] for label in item['labels']]
                }

                if include_body:
                    issue_data['body'] = item.get('body', '')

                issues.append(issue_data)

        return issues

    async def get_pull_requests(self, owner: str, repo: str, since: Optional[datetime] = None,
                               until: Optional[datetime] = None, state: str = "all",
                               per_page: int = 50, merged_only: bool = False,
                               include_body: bool = False) -> List[Dict]:
        """è·å–ä»“åº“çš„ pull requests åˆ—è¡¨"""
        url = f"{self.base_url}/repos/{owner}/{repo}/pulls"
        params = {
            'state': state,
            'per_page': per_page,
            'sort': 'updated',
            'direction': 'desc'
        }

        data = await self._make_request(url, params)

        pull_requests = []
        for item in data:
            # æ—¶é—´è¿‡æ»¤
            if since:
                updated_at = datetime.fromisoformat(item['updated_at'].replace('Z', '+00:00'))
                if updated_at < since:
                    continue

            if until:
                updated_at = datetime.fromisoformat(item['updated_at'].replace('Z', '+00:00'))
                if updated_at > until:
                    continue

            # å¦‚æœåªè¦mergedçš„PR
            if merged_only and not item.get('merged_at'):
                continue

            pr_data = {
                'number': item['number'],
                'title': item['title'],
                'state': item['state'],
                'user': item['user']['login'],
                'created_at': item['created_at'],
                'updated_at': item['updated_at'],
                'html_url': item['html_url'],
                'merged_at': item.get('merged_at'),
                'draft': item.get('draft', False),
                'base_branch': item['base']['ref'],
                'head_branch': item['head']['ref']
            }

            # å¯é€‰åŒ…å«bodyå†…å®¹
            if include_body and item.get('body'):
                # é™åˆ¶æè¿°é•¿åº¦ï¼Œå‡å°‘tokenä½¿ç”¨
                body = item['body'][:150] + "..." if len(item['body']) > 150 else item['body']
                pr_data['body'] = body

            pull_requests.append(pr_data)

        return pull_requests

    async def export_daily_progress(self, owner: str, repo: str,
                                   output_dir: str = "daily_progress",
                                   since: Optional[datetime] = None,
                                   until: Optional[datetime] = None,
                                   compact_mode: bool = True) -> str:
        """å¯¼å‡ºæ¯æ—¥è¿›å±•åˆ° Markdown æ–‡ä»¶"""
        # é»˜è®¤æ—¶é—´èŒƒå›´ï¼šè¿‡å»24å°æ—¶
        if not since:
            since = datetime.now(timezone.utc) - timedelta(hours=24)
        if not until:
            until = datetime.now(timezone.utc)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # æ ¹æ®æ¨¡å¼è°ƒæ•´å‚æ•°
        if compact_mode:
            # ç´§å‡‘æ¨¡å¼ï¼šåªè·å–merged PRå’Œclosed issuesï¼Œä¸åŒ…å«body
            issues = await self.get_issues(
                owner, repo, since=since, until=until,
                state="closed", per_page=20, include_body=False
            )
            pull_requests = await self.get_pull_requests(
                owner, repo, since=since, until=until,
                per_page=20, merged_only=True, include_body=False
            )
        else:
            # å®Œæ•´æ¨¡å¼ï¼šè·å–æ‰€æœ‰çŠ¶æ€çš„issueså’ŒPR
            issues = await self.get_issues(
                owner, repo, since=since, until=until,
                state="all", per_page=50, include_body=True
            )
            pull_requests = await self.get_pull_requests(
                owner, repo, since=since, until=until,
                state="all", per_page=50, merged_only=False, include_body=True
            )

        # ç”Ÿæˆæ–‡ä»¶å
        date_str = until.strftime("%Y%m%d")
        filename = f"{repo}_{date_str}.md"
        filepath = output_path / filename

        # ç”Ÿæˆ Markdown å†…å®¹
        content = self._generate_markdown_content(
            owner, repo, issues, pull_requests, since, until, compact_mode
        )

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.info(f"æ¯æ—¥è¿›å±•æŠ¥å‘Šå·²å¯¼å‡º: {filepath}")
        return str(filepath)

    def _generate_markdown_content(self, owner: str, repo: str,
                                  issues: List[Dict], pull_requests: List[Dict],
                                  since: datetime, until: datetime,
                                  compact_mode: bool) -> str:
        """ç”Ÿæˆ Markdown å†…å®¹"""
        date_str = until.strftime("%Y-%m-%d")
        time_range = f"{since.strftime('%Y-%m-%d %H:%M')} è‡³ {until.strftime('%Y-%m-%d %H:%M')} (UTC)"
        mode_str = "ç´§å‡‘æ¨¡å¼" if compact_mode else "å®Œæ•´æ¨¡å¼"

        content = f"""# {owner}/{repo} - æ¯æ—¥è¿›å±•æŠ¥å‘Š

**æ—¥æœŸ**: {date_str}  
**æ—¶é—´èŒƒå›´**: {time_range}  
**æ¨¡å¼**: {mode_str}  
**ç”Ÿæˆæ—¶é—´**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC

---

## ğŸ“Š æ¦‚è§ˆ

- **Pull Requests**: {len(pull_requests)} ä¸ª{'å·²åˆå¹¶' if compact_mode else ''}
- **Issues**: {len(issues)} ä¸ª{'å·²å…³é—­' if compact_mode else ''}

---

## ğŸ”€ Pull Requests {f'(å·²åˆå¹¶)' if compact_mode else ''}

"""

        if pull_requests:
            for pr in pull_requests:
                status_icon = "âœ…" if pr.get('merged_at') else ("ğŸ”€" if pr['state'] == 'open' else "âŒ")
                merge_info = f" - åˆå¹¶æ—¶é—´: {pr['merged_at']}" if pr.get('merged_at') else ""
                draft_info = " ğŸ“" if pr.get('draft') else ""

                content += f"### {status_icon} #{pr['number']} {pr['title']}{draft_info}\n\n"
                content += f"- **ä½œè€…**: {pr['user']}\n"
                content += f"- **çŠ¶æ€**: {pr['state']}\n"
                content += f"- **åˆ†æ”¯**: {pr['head_branch']} â†’ {pr['base_branch']}\n"
                content += f"- **åˆ›å»ºæ—¶é—´**: {pr['created_at']}\n"
                if merge_info:
                    content += merge_info + "\n"
                content += f"- **é“¾æ¥**: [{pr['html_url']}]({pr['html_url']})\n"

                if not compact_mode and pr.get('body'):
                    content += f"- **æè¿°**: {pr['body']}\n"

                content += "\n"
        else:
            content += f"æ— {'å·²åˆå¹¶' if compact_mode else ''}çš„ Pull Requests\n\n"

        content += f"""---

## ğŸ› Issues {f'(å·²å…³é—­)' if compact_mode else ''}

"""

        if issues:
            for issue in issues:
                status_icon = "âœ…" if issue['state'] == 'closed' else "ğŸ”´"
                labels_info = f" ğŸ·ï¸ {', '.join(issue['labels'])}" if issue.get('labels') else ""

                content += f"### {status_icon} #{issue['number']} {issue['title']}{labels_info}\n\n"
                content += f"- **ä½œè€…**: {issue['user']}\n"
                content += f"- **çŠ¶æ€**: {issue['state']}\n"
                content += f"- **åˆ›å»ºæ—¶é—´**: {issue['created_at']}\n"
                content += f"- **æ›´æ–°æ—¶é—´**: {issue['updated_at']}\n"
                content += f"- **é“¾æ¥**: [{issue['html_url']}]({issue['html_url']})\n"

                if not compact_mode and issue.get('body'):
                    content += f"- **æè¿°**: {issue['body']}\n"

                content += "\n"
        else:
            content += f"æ— {'å·²å…³é—­' if compact_mode else ''}çš„ Issues\n\n"

        content += """---

## ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯

"""
        if compact_mode:
            content += f"- å·²åˆå¹¶ PR: {len(pull_requests)}\n"
            content += f"- å·²å…³é—­ Issues: {len(issues)}\n"
        else:
            # å®Œæ•´æ¨¡å¼çš„ç»Ÿè®¡
            merged_prs = len([pr for pr in pull_requests if pr.get('merged_at')])
            open_prs = len([pr for pr in pull_requests if pr['state'] == 'open'])
            closed_prs = len([pr for pr in pull_requests if pr['state'] == 'closed' and not pr.get('merged_at')])

            open_issues = len([issue for issue in issues if issue['state'] == 'open'])
            closed_issues = len([issue for issue in issues if issue['state'] == 'closed'])

            content += f"- å·²åˆå¹¶ PR: {merged_prs}\n"
            content += f"- å¼€æ”¾ PR: {open_prs}\n"
            content += f"- å·²å…³é—­ PR: {closed_prs}\n"
            content += f"- å¼€æ”¾ Issues: {open_issues}\n"
            content += f"- å·²å…³é—­ Issues: {closed_issues}\n"

        content += f"\n**æŠ¥å‘Šç”Ÿæˆå·¥å…·**: GitHub Sentinel v0.2\n"

        return content
