"""
GitHub API æœåŠ¡
"""
import aiohttp
import asyncio
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta, timezone
import logging
import json
import os
from pathlib import Path

from ..models.repository import Repository, RepositoryUpdate


class GitHubService:
    """GitHub API æœåŠ¡ç±»"""

    def __init__(self, token: str, rate_limit_per_hour: int = 5000, timeout: int = 30):
        self.token = token
        self.base_url = "https://api.github.com"
        self.headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
            "User-Agent": "GitHub-Sentinel/1.0"
        }
        self.rate_limit_per_hour = rate_limit_per_hour
        self.requests_made = 0
        self.last_reset = datetime.now()
        self.timeout = timeout
        self.logger = logging.getLogger(__name__)

    async def _check_rate_limit(self):
        """æ£€æŸ¥APIé€Ÿç‡é™åˆ¶"""
        now = datetime.now()
        if (now - self.last_reset).total_seconds() > 3600:
            self.requests_made = 0
            self.last_reset = now

        if self.requests_made >= self.rate_limit_per_hour:
            wait_time = 3600 - (now - self.last_reset).total_seconds()
            if wait_time > 0:
                self.logger.warning(f"è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.0f} ç§’")
                await asyncio.sleep(wait_time)
                self.requests_made = 0
                self.last_reset = datetime.now()

    async def _make_request(self, url: str, params: Optional[Dict] = None) -> Dict:
        """å‘èµ·APIè¯·æ±‚"""
        await self._check_rate_limit()

        timeout = aiohttp.ClientTimeout(total=self.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                async with session.get(url, headers=self.headers, params=params) as response:
                    self.requests_made += 1

                    if response.status == 200:
                        return await response.json()
                    elif response.status == 403:
                        self.logger.error(f"APIè®¿é—®è¢«æ‹’ç»: {response.status}")
                        raise Exception(f"GitHub APIè®¿é—®è¢«æ‹’ç»: {response.status}")
                    elif response.status == 404:
                        self.logger.error(f"èµ„æºæœªæ‰¾åˆ°: {url}")
                        raise Exception(f"GitHubèµ„æºæœªæ‰¾åˆ°: {url}")
                    else:
                        self.logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status}")
                        raise Exception(f"GitHub APIè¯·æ±‚å¤±è´¥: {response.status}")

            except asyncio.TimeoutError:
                self.logger.error(f"è¯·æ±‚è¶…æ—¶: {url}")
                raise Exception(f"GitHub APIè¯·æ±‚è¶…æ—¶: {url}")
            except Exception as e:
                self.logger.error(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")
                raise

    async def get_repository_info(self, owner: str, repo: str) -> Repository:
        """è·å–ä»“åº“åŸºæœ¬ä¿¡æ¯"""
        url = f"{self.base_url}/repos/{owner}/{repo}"
        data = await self._make_request(url)

        return Repository(
            name=data['name'],
            full_name=data['full_name'],
            description=data.get('description', ''),
            html_url=data['html_url'],
            language=data.get('language', ''),
            stars=data['stargazers_count'],
            forks=data['forks_count'],
            updated_at=datetime.fromisoformat(data['updated_at'].replace('Z', '+00:00')),
            created_at=datetime.fromisoformat(data['created_at'].replace('Z', '+00:00'))
        )

    async def get_repository_updates(self, owner: str, repo: str, since: datetime) -> List[RepositoryUpdate]:
        """è·å–ä»“åº“æ›´æ–°ä¿¡æ¯"""
        # è·å–æœ€æ–°çš„commits
        commits_url = f"{self.base_url}/repos/{owner}/{repo}/commits"
        params = {
            'since': since.isoformat(),
            'per_page': 100
        }

        commits_data = await self._make_request(commits_url, params)

        updates = []
        for commit in commits_data:
            updates.append(RepositoryUpdate(
                type='commit',
                title=commit['commit']['message'].split('\n')[0],
                description=commit['commit']['message'],
                author=commit['commit']['author']['name'],
                created_at=datetime.fromisoformat(commit['commit']['author']['date'].replace('Z', '+00:00')),
                html_url=commit['html_url']
            ))

        return updates

    async def get_issues(self, owner: str, repo: str, since: Optional[datetime] = None,
                        until: Optional[datetime] = None, state: str = "all",
                        per_page: int = 50, include_body: bool = False) -> List[Dict]:
        """è·å–ä»“åº“çš„ issues åˆ—è¡¨"""
        url = f"{self.base_url}/repos/{owner}/{repo}/issues"
        params = {
            'state': state,
            'per_page': per_page,
            'sort': 'updated',
            'direction': 'desc'
        }

        if since:
            params['since'] = since.isoformat()

        data = await self._make_request(url, params)

        # è¿‡æ»¤æ‰ pull requests (GitHub API ä¸­ issues åŒ…å« pull requests)
        issues = []
        for item in data:
            if 'pull_request' not in item:
                # æ—¶é—´è¿‡æ»¤
                if until:
                    updated_at = datetime.fromisoformat(item['updated_at'].replace('Z', '+00:00'))
                    if updated_at > until:
                        continue

                issue_data = {
                    'number': item['number'],
                    'title': item['title'],
                    'state': item['state'],
                    'user': item['user']['login'],
                    'created_at': item['created_at'],
                    'updated_at': item['updated_at'],
                    'html_url': item['html_url'],
                    'labels': [label['name'] for label in item.get('labels', [])]
                }

                # å¯é€‰åŒ…å«bodyå†…å®¹
                if include_body and item.get('body'):
                    # é™åˆ¶æè¿°é•¿åº¦ï¼Œå‡å°‘tokenä½¿ç”¨
                    body = item['body'][:150] + "..." if len(item['body']) > 150 else item['body']
                    issue_data['body'] = body

                issues.append(issue_data)

        return issues

    async def get_pull_requests(self, owner: str, repo: str, since: Optional[datetime] = None,
                               until: Optional[datetime] = None, state: str = "all",
                               per_page: int = 50, merged_only: bool = False,
                               include_body: bool = False) -> List[Dict]:
        """è·å–ä»“åº“çš„ pull requests åˆ—è¡¨"""
        url = f"{self.base_url}/repos/{owner}/{repo}/pulls"
        params = {
            'state': state,
            'per_page': per_page,
            'sort': 'updated',
            'direction': 'desc'
        }

        data = await self._make_request(url, params)

        pull_requests = []
        for item in data:
            # æ—¶é—´è¿‡æ»¤
            if since:
                updated_at = datetime.fromisoformat(item['updated_at'].replace('Z', '+00:00'))
                if updated_at < since:
                    continue

            if until:
                updated_at = datetime.fromisoformat(item['updated_at'].replace('Z', '+00:00'))
                if updated_at > until:
                    continue

            # å¦‚æœåªè¦mergedçš„PR
            if merged_only and not item.get('merged_at'):
                continue

            pr_data = {
                'number': item['number'],
                'title': item['title'],
                'state': item['state'],
                'user': item['user']['login'],
                'created_at': item['created_at'],
                'updated_at': item['updated_at'],
                'html_url': item['html_url'],
                'merged_at': item.get('merged_at'),
                'draft': item.get('draft', False),
                'base_branch': item['base']['ref'],
                'head_branch': item['head']['ref']
            }

            # å¯é€‰åŒ…å«bodyå†…å®¹
            if include_body and item.get('body'):
                # é™åˆ¶æè¿°é•¿åº¦ï¼Œå‡å°‘tokenä½¿ç”¨
                body = item['body'][:150] + "..." if len(item['body']) > 150 else item['body']
                pr_data['body'] = body

            pull_requests.append(pr_data)

        return pull_requests

    async def export_daily_progress(self, owner: str, repo: str,
                                   output_dir: str = "daily_progress",
                                   since: Optional[datetime] = None,
                                   until: Optional[datetime] = None,
                                   compact_mode: bool = True) -> str:
        """å¯¼å‡ºæ¯æ—¥è¿›å±•åˆ° Markdown æ–‡ä»¶"""
        # é»˜è®¤æ—¶é—´èŒƒå›´ï¼šè¿‡å»24å°æ—¶
        if not since:
            since = datetime.now(timezone.utc) - timedelta(hours=24)
        if not until:
            until = datetime.now(timezone.utc)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # æ ¹æ®æ¨¡å¼è°ƒæ•´å‚æ•°
        if compact_mode:
            # ç´§å‡‘æ¨¡å¼ï¼šåªè·å–merged PRå’Œopen issuesï¼Œä¸åŒ…å«body
            issues = await self.get_issues(
                owner, repo, since=since, until=until,
                state="open", per_page=20, include_body=False
            )
            pull_requests = await self.get_pull_requests(
                owner, repo, since=since, until=until,
                per_page=20, merged_only=True, include_body=False
            )
        else:
            # å®Œæ•´æ¨¡å¼
            issues = await self.get_issues(
                owner, repo, since=since, until=until,
                per_page=50, include_body=True
            )
            pull_requests = await self.get_pull_requests(
                owner, repo, since=since, until=until,
                per_page=50, include_body=True
            )

        # ç”Ÿæˆæ–‡ä»¶å
        date_str = until.strftime("%Y%m%d")
        filename = f"{repo}_{date_str}.md"
        filepath = output_path / filename

        # ç”Ÿæˆ Markdown å†…å®¹
        markdown_content = self._generate_progress_markdown(
            repo, owner, issues, pull_requests, since, until, compact_mode
        )

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        self.logger.info(f"æ¯æ—¥è¿›å±•æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {filepath}")
        return str(filepath)

    def _generate_progress_markdown(self, repo: str, owner: str, issues: List[Dict],
                                   pull_requests: List[Dict], since: datetime,
                                   until: datetime, compact_mode: bool = True) -> str:
        """ç”Ÿæˆè¿›å±• Markdown å†…å®¹"""
        date_range = f"{since.strftime('%Y-%m-%d %H:%M')} è‡³ {until.strftime('%Y-%m-%d %H:%M')}"

        content = f"""# {repo} é¡¹ç›®è¿›å±•æŠ¥å‘Š

## ğŸ“… æ—¶é—´èŒƒå›´ï¼š{date_range}

"""

        # Pull Requests éƒ¨åˆ†ï¼ˆä¼˜å…ˆæ˜¾ç¤ºï¼Œå› ä¸ºæ›´é‡è¦ï¼‰
        if compact_mode:
            content += f"## âœ… å·²åˆå¹¶ Pull Requests ({len(pull_requests)} ä¸ª)\n\n"
        else:
            content += f"## ğŸ”„ Pull Requests æ›´æ–° ({len(pull_requests)} ä¸ª)\n\n"

        if pull_requests:
            for pr in pull_requests:
                content += f"### #{pr['number']} {pr['title']}\n"
                content += f"- **çŠ¶æ€**: {pr['state']}"
                if pr['merged_at']:
                    merged_time = datetime.fromisoformat(pr['merged_at'].replace('Z', '+00:00'))
                    content += f" (å·²åˆå¹¶: {merged_time.strftime('%m-%d %H:%M')})"
                content += "\n"
                content += f"- **ä½œè€…**: {pr['user']}\n"
                content += f"- **åˆ†æ”¯**: `{pr['head_branch']}` â†’ `{pr['base_branch']}`\n"

                if not compact_mode:
                    content += f"- **é“¾æ¥**: [{pr['html_url']}]({pr['html_url']})\n"
                    if pr.get('body'):
                        content += f"- **æè¿°**: {pr['body']}\n"
                content += "\n"
        else:
            content += "æš‚æ— ç›¸å…³ Pull Requests\n\n"

        # Issues éƒ¨åˆ†
        if compact_mode:
            content += f"## ğŸ› å¾…å¤„ç† Issues ({len(issues)} ä¸ª)\n\n"
        else:
            content += f"## ğŸ“‹ Issues æ›´æ–° ({len(issues)} ä¸ª)\n\n"

        if issues:
            for issue in issues:
                content += f"### #{issue['number']} {issue['title']}\n"
                content += f"- **çŠ¶æ€**: {issue['state']}\n"
                content += f"- **åˆ›å»ºè€…**: {issue['user']}\n"

                if not compact_mode:
                    content += f"- **æ›´æ–°æ—¶é—´**: {issue['updated_at']}\n"
                    content += f"- **é“¾æ¥**: [{issue['html_url']}]({issue['html_url']})\n"

                if issue.get('labels'):
                    content += f"- **æ ‡ç­¾**: {', '.join(issue['labels'])}\n"

                if not compact_mode and issue.get('body'):
                    content += f"- **æè¿°**: {issue['body']}\n"
                content += "\n"
        else:
            content += "æš‚æ— ç›¸å…³ Issues\n\n"

        # ç»Ÿè®¡ä¿¡æ¯
        content += f"""## ğŸ“Š ç»Ÿè®¡æ‘˜è¦

- **æ—¶é—´èŒƒå›´**: {(until - since).total_seconds() / 3600:.1f} å°æ—¶
- **å·²åˆå¹¶ PR**: {len([pr for pr in pull_requests if pr.get('merged_at')])} ä¸ª
- **å¾…å¤„ç† Issues**: {len([issue for issue in issues if issue['state'] == 'open'])} ä¸ª

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*æ¨¡å¼: {'ç´§å‡‘æ¨¡å¼' if compact_mode else 'è¯¦ç»†æ¨¡å¼'}*
"""

        return content
